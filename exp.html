<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Adaptive Concretization for Parallel Program Synthesis</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
  <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  <link rel="stylesheet" href="css/github-prettify-theme.css">
  <script src="js/site.js"></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">
</head>
<body class="code-snippets-visible">

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="index.html#intro">Intro</a></li>
          <li class="navbar-item">
            <a class="navbar-link" href="#" data-popover="#downloadPopover">Downloads</a>
            <div id="downloadPopover" class="popover">
              <ul class="popover-list">
                <li class="popover-item">
                  <a class="popover-link" href="index.html#OVA">OVA</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="index.html#tar-ball">tar ball</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="index.tml#source">source</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#usage">usage</a>
                </li>
              </ul>
            </div>
          </li>
          <li class="navbar-item">
            <a class="navbar-link" href="#" data-popover="#expNavPopover">Experiments</a>
            <div id="expNavPopover" class="popover">
              <ul class="popover-list">
                <li class="popover-item">
                  <a class="popover-link" href="#infra">infra</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#DB">database</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#exp-short">short</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#exp-full">full</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#exp-raw">raw data</a>
                </li>
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </nav>

    <div class="docs-section" id="infra">
      <h5 class="docs-header">Testing Infrastructure</h5>
      <p>
Download the
<a href="https://github.com/plum-umd/adaptive-concretization/archive/master.zip">archive</a>
of our testing infrastructure or clone it as follows:
      </p>
<pre><code>.../ae $ git clone https://github.com/plum-umd/adaptive-concretization.git
</code></pre>
      <p>
This infrastructure is tested under Python 2.7.x.
If you are using Python 3.2/3.3, replace the usage of <code>subprocess32</code>
at line 8 in <code>psketch.py</code> with <code>subprocess</code>.
If you are using Python 2.x and want to examine the back-end behavior for
various randdegrees, which corresponds to Table 1 in the paper, you need to install
<a href="https://pypi.python.org/pypi/subprocess32/">subprocess32</a>.
If you are not using Python 2.7.9 or Python 3.4 and later, you need to install
<a href="https://pip.pypa.io/en/latest/installing.html">pip</a> first.
      </p>
      <p>
On Debian and Ubuntu,
      </p>
<pre><code>$ sudo apt-get install python-dev python-pip
</code></pre>
<!--
      <p>
On Mac OS X,
      </p>
<pre><code>$ sudo easy_install pip
</code></pre>
      <p>
On other OSes, download <a href="https://bootstrap.pypa.io/get-pip.py">get-pip.py</a>
and run the following (which may require administrator access):
      </p>
<pre><code>$ (sudo) python get-pip.py
</code></pre>
      <p>
--!>
Once <code>pip</code> is ready, install <code>subprocess32</code>:
      </p>
<pre><code>$ sudo pip install subprocess32
</code></pre>
      <p>
To post-analyze Sketch output, we use
<a href="http://www.scipy.org/install.html">SciPy</a>.
      </p>
      <p>
On Debian and Ubuntu,
      </p>
<pre><code>sudo apt-get install python-numpy python-scipy python-matplotlib
</code></pre>
<!--
      <p>
On Mac OS X,
      </p>
<pre><code>sudo port install py27-numpy py27-scipy py27-matplotlib
</code></pre>
-->
    </div>

    <div class="docs-section" id="DB">
      <h5 class="docs-header">Database setup</h5>
      <p>
We thoroughly tested our algorithm by running Sketch over 7K times
across different sets of benchmarks, different degrees,
and different numbers of cores.  Needless to say, it might be very
painful if we run test cases; post-analyze outputs; and depict tables,
like Tables 2 and 3 in the paper, <em>by hand</em>.
The main purpose of this testing infrastructure is to conduct
intensive experimental evaluations in an <em>automated</em> way.
Our first step towards such automation is using database
to save and retrieve experiment results easily.
(You can still see raw data under data/ folder.)
      </p>
      <p>
On Debian and Ubuntu,
      </p>
<pre><code>$ sudo apt-get install mysql-server mysql-client
</code></pre>
<!--
      <p>
On Mac OS X,
      </p>
<pre><code>$ sudo port install mysql5
</code></pre>
-->
      <p>
The prompt (or install GUI) will ask you for the current root password.
Type whatever password you want.  (Note that this will be used later.)
      </p>
<pre><code>Enter current password for root (enter for none):

OK, successfully used password, moving on...
</code></pre>
      <p>
Once you've installed MySQL, you need to activate it and
run the MySQL set up script:
      </p>
<pre><code>$ sudo mysql_install_db
$ sudo /usr/bin/mysql_secure_installation
</code></pre>
      <p>
Then, the prompt will ask several options you can configure.
Since we are using this database for experiment purpose only,
we recommend you to set it up in the most secure manner,
for example, no anonymous access and no remote access;
the easiest way is just to say Yes to all options
(except for changing the root password):
      </p>
<pre><code>...
Remove anonymous users? [Y/n] y
 ... Success!

...
Disallow root login remotely? [Y/n] y
 ... Success!

...
Remove test database and access to it? [Y/n] y
 - Dropping test database...
 ... Success!
 - Removing privileges on test database...
 ... Success!

...
Reload privilege tables now? [Y/n] y
 ... Success!
</code></pre>
      <p>
Access to the db server and create an account for this experiment:
      </p>
<pre><code>$ mysql --user=root -p -h 127.0.0.1 mysql
password: *******
mysql> CREATE USER 'sketchperf'@'localhost';
mysql> GRANT ALL PRIVILEGES ON *.* TO 'sketchperf'@'localhost';
</code></pre>
      <p>
Access to the db server with that account and generate a database:
      </p>
<pre><code>$ mysql --user=sketchperf -h 127.0.0.1
mysql> CREATE DATABASE concretization;
mysql> SHOW DATABASES;
</code></pre>
      <p>
From now on, you can connect to that database like this:
      </p>
<pre><code>$ mysql --user=sketchperf -h 127.0.0.1 concretization
</code></pre>
      <p>
In case you want to use different account and database names,
use options <code>--user</code> and <code>--db</code>
in <code>db.py</code> accordingly.
      </p>
<pre><code>.../ae/adaptive-concretization $ ./db.py --user user_name --db db_name ...
</code></pre>
      <p>
To connect to the database, you need to install
<a href="https://www.mysql.com/products/connector/">MySQL connector</a>.
      </p>
      <p>
On Debian and Ubuntu,
      </p>
<pre><code>$ sudo apt-get install python-mysql.connector
</code></pre>
<!--
      <p>
On Mac OS X,
      </p>
<pre><code>$ sudo port install py-mysql
</code></pre>
-->
      <p>
Inside the database, we will have the following tables:
<ol>
  <li>Experiment: groups each experiment</li>
  <li>RunS: records single-threaded runs (for Table 1)</li>
  <li>RunP: records (strategical) parallel runs (for Tables 2 and 3)</li>
  <li>Dag: records DAG sizes for every harness</li>
  <li>Hole: records hole concretization</li>
</ol>
Experiment is simply a pair of EID and RID,
<em>i.e.</em>, Runs that have the same EID are grouped as an experiment.
All statistics will be calculated per experiment.
Dag and Hole are connected to RunS via RID; these were used
for our preliminary experiments, and not included in the paper.
Refer to comments around lines
<a href="https://github.com/plum-umd/adaptive-concretization/blob/master/db.py#L21">21</a>--45 in <code>db.py</code>
to see the schemas for aforementioned experiment tables.
      </p>
      <p>
You can easily re/define and clean tables in that database:
      </p>
<pre><code>.../ae/adaptive-concretization $ ./db.py -c init [-v]
.../ae/adaptive-concretization $ ./db.py -c clean [-v]
.../ae/adaptive-concretization $ ./db.py -c reset [-v]
</code></pre>
      <p>
You can also see detailed queries when running <code>db.py</code> with
verbosity option <code>-v</code>.
(This option is applicable for all the other commands explained below.)
All those commands are guarded by either <code>IF EXISTS</code> or
<code>IF NOT EXISTS</code>, and thus it is safe to execute under
any conditions.
      </p>

    </div>

    <div class="docs-section" id="exp-short">
      <h5 class="docs-header">Experiment (short version)</h5>
      <p>
There is a bunch of options in <code>run.py</code> through which you can specify
benchmark(s), number of cores, randdegree(s) of interest, and which strategies
(such as vanilla Sketch, fixed randdegree, adaptive concretization) to use.
For your (and our) convenience, there are configuration files,
where we can literally configure all the experiments we want to conduct.
<code>config.json</code> (for short version) is a subset of
<code>config.full.json</code> (for full version of experiments, respectively),
and <code>run.py</code> uses <code>config.json</code> by default.
One thing that is missed in those configuration files is
the number of times to repeat the experiments: <code>-r #n</code>.
      </p>
      <p>
All Sketch output files will be placed under <code>data/</code> folder.
You can check those outputs by running <code>post.py</code>;
register those post-analyzed outputs to the database via
<code>-c register</code> command in <code>db.py</code>;
or run and register at the same time
(<code>--register</code> option in <code>run.py</code>).
Once the experiments are done, you can calculate the statistics
via <code>-c stat</code> command in <code>db.py</code>.
(All detailed uses will be explained again with concrete examples.)
      </p>
      <p>
If you are running only the experiment specified in the configuration,
just follow these instructions:
      </p>
<pre><code>.../ae/adaptive-concretization $ ./clean.sh
.../ae/adaptive-concretization $ ./run.py -r 13
.../ae/adaptive-concretization $ ./run.py -r 13 --vanilla
.../ae/adaptive-concretization $ ./db.py -c register -s
.../ae/adaptive-concretization $ ./db.py -c register
.../ae/adaptive-concretization $ ./db.py -c stat -s
.../ae/adaptive-concretization $ ./db.py -c stat
</code></pre>
      <p>
The second-to-last command will retrieve the statistics for Table 1,
while the last command will show you the statistics for Tables 2 and 3.
For non-adaptive concretization, which is the last column in Table 2,
please refer to the last part of the experiments below.
The following instructions will explain detailed experiments step by step.
      </p>
      <p>
The first experiment is about examining Sketch back-end behavior with various
randdegrees, to show the hypothesis that optimal randdegree varies
from benchmark to benchmark.  <code>run.py</code> has an option, <code>-s</code>,
that runs back-end instances in parallel
via <code>python</code> wrapper: <code>psketch.py</code>.
That wrapper simply forks the certain number of back-end instances that you specify,
and allows them to run without any interruptions.
That is, it will <strong>not</strong> do anything clever to manage them,
e.g., stopping all other instances if one found a solution.
(On the other hand, parallel running via Sketch front-end, which will be used for
all the other experiments below, manages the instances of back-end and
stops all active instances if one found a solution.)
In this regard, we can properly collect empirical success rates.
      </p>
<pre><code>.../ae/adaptive-concretization $ ./clean.sh
.../ae/adaptive-concretization $ ./run.py -s -r #n
</code></pre>
      <p>
You can run particular benchmarks with particular degrees:
      </p>
<pre><code>.../ae/adaptive-concretization $ ./run.py -s -r #n [-b name]* [-d degree]*
.../ae/adaptive-concretization $ ./run.py -s -r 1024 -b p_color -b deriv2 -d 64 -d 1024
</code></pre>
      <p>
Check and register Sketch outputs as follows.
(Notice that <code>-s</code> option is required for other scripts.)
      </p>
<pre><code>.../ae/adaptive-concretization $ ./post.py -s
.../ae/adaptive-concretization $ ./db.py -c register -s [-e EID]
</code></pre>
      <p>
As mentioned earlier, you can run and register at the same time,
which is applicable to all other experiments below:
      </p>
<pre><code>.../ae/adaptive-concretization $ ./run.py -s -r #n --register
</code></pre>
      <p>
You can retrieve the statistics from the database:
      </p>
<pre><code>.../ae/adaptive-concretization $ ./db.py -c stat -s [-e EID] [--detail] [-v]
</code></pre>
      <p>
The next experiment is about parallel scalability of our algorithm.
To obtain statistics about plain Sketch (as in Tables 2 and 3),
use <code>--vanilla</code> option in <code>run.py</code>.
      </p>
<pre><code>.../ae/adaptive-concretization $ ./run.py --vanilla -r #n [...]
.../ae/adaptive-concretization $ ./post.py
.../ae/adaptive-concretization $ ./db.py -c register [-e EID]
.../ae/adaptive-concretization $ ./db.py -c state [-e EID]
</code></pre>
      <p>
To obtain statistics about our algorithm,
specify the strategy: <code>--strategy WILCOXON</code>.
Otherwise, it will repeat the first experiment using Sketch front-end's parallel running.
(This happens in order to encompass all experiments settings in one.)
      </p>
<pre><code>.../ae/adaptive-concretization $ ./run.py --strategy WILCOXON -r #n [...]
(post-analysis, registration, and statistics steps are same as above.)
</code></pre>
      <p>
From the statistics, you can learn which degree was mostly chosen at the adaption phase.
To test the efficiency of the adaption phase,
we can run Sketch with that particular degree, assuming we already knew it:
      </p>
<pre><code>.../ae/adaptive-concretization $ ./run.py --strategy NOT_SET -d degree -r #n [...]
(post-analysis, registration, and statistics steps are same as above.)
</code></pre>
    </div>

    <div class="docs-section" id="exp-full">
      <h5 class="docs-header">Experiment (full version)</h5>
      <p>
The only difference between short and full version of experiments is
how many (big) benchmarks are used.
(If you skipped the previous section, please go back and read through it.)
Simply, <code>config.full.json</code> includes all benchmarks we've used
for our paper.  To make <code>run.py</code> use a different configuration
file, you need to pass the following option:
<code>--config config.full.json</code>.  For example,
      </p>
<pre><code>.../ae/adaptive-concretization $ ./run.py --config config.full.json -r #n [--timeout 30] [...]
(post-analysis, registration, and statistics steps are same as above.)
</code></pre>
      <p>
Notice that you may need to set timeout because Sketch might take too long in some cases
(e.g., plain Sketch for <code>p_menu</code>).
In sum, the simplest way to conduct the experiments is using the configuration
as follows:
      </p>
<pre><code>.../ae/adaptive-concretization $ ./clean.sh
.../ae/adaptive-concretization $ ./run.py --config config.full.json -r 13
.../ae/adaptive-concretization $ ./run.py --config config.full.json -r 13 --vanilla --timeout 30
.../ae/adaptive-concretization $ ./db.py -c register -s
.../ae/adaptive-concretization $ ./db.py -c register
.../ae/adaptive-concretization $ ./db.py -c stat -s
.../ae/adaptive-concretization $ ./db.py -c stat
</code></pre>

    </div>

    <div class="docs-section" id="exp-raw">
      <h5 class="docs-header">Raw Data</h5>
      <p>
You can download our database dumps and check data inside it:
      </p>
<pre><code>$ mysql -u username [-p] new_db_name < dump.sql
</code></pre>
<ol>
  <li>CAV 2015 submission (as of Feb 5, 2015): <a href="http://people.csail.mit.edu/jsjeon/adaptive-concretization/cav2015_submission.sql">cav2015_submission.sql</a></li>
  <li>CAV 2015 artifact evaluation (as of Apr 24, 2015): <a href="http://people.csail.mit.edu/jsjeon/adaptive-concretization/cav2015_ae.sql">cav2015_ae.sql</a></li>
</ol>
    </div>

  </div>
</body>
</html>
